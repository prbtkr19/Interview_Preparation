{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file operations in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file operation takes place in the following order:\n",
    "\n",
    "-   1.Open a file\n",
    "-   2.Read or write (perform operation)\n",
    "-   3.Close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ,this is just a test message to print\n"
     ]
    }
   ],
   "source": [
    "filepath=\"test.txt\"\n",
    "# Open the file in read mode ('r')\n",
    "#with open(filepath, 'r', encoding='utf-8') as file:\n",
    "with open(filepath, 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    file_content = file.read()\n",
    "    \n",
    "# Print or process the content as needed\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opening a text and writing content in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   it will create filename if does not exit.\n",
    "-   it will delete previous content and replace with new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\",'w') as w2:\n",
    "    text=\"this is the third line added to text\"\n",
    "    w2.write(text)\n",
    "    w2.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text\n",
    "with open(\"test1_mar04.txt\",\"w\") as w1:\n",
    "    text=\"The extract_frames_and_detect_objects function continuously reads frames from a video using FFmpeg's stdout pipe.Make sure to change the path to the Haar cascade classifier XML file based on your OpenCV installation.\"    \n",
    "    w1.write(text)\n",
    "    w1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the file pointer to a specific byte offset (e.g., 20 bytes from the beginning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nd_detect_objects function continuously reads frames from a video using FFmpeg's stdout pipe.Make sure to change the path to the Haar cascade classifier XML file based on your OpenCV installation.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading above write text\n",
    "import shutil\n",
    "with open(\"test1_mar04.txt\",'r') as r:\n",
    "\n",
    "    # Move the file pointer to a specific byte offset (e.g., 20 bytes from the beginning)\n",
    "    r.seek(20)\n",
    "    data_inside=r.read()\n",
    "    \n",
    "    # copying original text\n",
    "    shutil.copy(\"test1_mar04.txt\",\"new_files_mar_04.txt\")\n",
    "    \n",
    "    \n",
    "data_inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffered read\n",
    "-   io : library in Python provides tools for working with streams of data\n",
    "\n",
    "\n",
    "-   StringIO and BytesIO classes allow you to treat strings and bytes respectively as file-like objects.\n",
    "-   These are useful for simulating file I/O operations in memory without using actual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "binary stem-----<_io.BytesIO object at 0x7f056472c3b0>\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Using StringIO to write and read strings\n",
    "output = io.StringIO()\n",
    "output.write('Hello, ')\n",
    "output.write('world!')\n",
    "contents = output.getvalue()\n",
    "output.close()\n",
    "print(contents)  \n",
    "# Using BytesIO for binary data\n",
    "data = b'Some binary data'\n",
    "binary_stream = io.BytesIO(data)\n",
    "print(f\"binary stem-----{binary_stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Libraries like dask, pandas, and Apache Spark are designed for scalable data processing.\n",
    "\n",
    "They provide abstractions for processing large datasets\n",
    "efficiently by partitioning data and performing operations in a distributed manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   buffering=1024*1024 is an optional parameter used in the open() function. \n",
    "-   It specifies the buffer size in bytes for reading and writing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case, a buffer size of 1 MB (1024 * 1024 bytes) is set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Buffering </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Buffering is a technique used to optimize input/output operations by reducing the number of system calls.\n",
    "-   When buffering is enabled, data is read into memory in larger chunks, improving efficiency.\n",
    "\n",
    "-   file.read(4096) reads the next 4096 bytes (4 KB) from the file object file. \n",
    "-   This function call is used inside the iter() function,\n",
    "\n",
    "\n",
    "which creates an iterator that repeatedly calls the function until it returns a specified value, in this case, an empty string ''. \n",
    "The iteration stops when file.read(4096) returns an empty string, indicating that the end of the file has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can perform these actions using the AWS console, the AWS SDKs, the SageMaker Python SDK, AWS CloudFormation or the AWS CLI.\n",
      "\n",
      "For batch inference with batch transform, point to your model artifacts and input data and create a batch inference job. Instead of hosting an endpoint for inference, SageMaker outputs your inferences to an Amazon S3 location of your choice.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading large volume of text data into chunks\n",
    "filepath=\"large_text_file.txt\"\n",
    "with open(filepath, 'r', buffering=1024*1024) as file:\n",
    "    for chunk in iter(lambda: file.read(2096), ''):\n",
    "        #process(chunk) # here process is the the custom function which perform some action on strings\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffered write\n",
    "-   if you want to read large vol of data in chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "with open(\"test.txt\",\"wb\") as file:\n",
    "    f1=io.BufferedWriter(file)\n",
    "    file.write(b\"this is bufered write\\n\")\n",
    "    file.write(b\"this is secon dline trying to write bufered write\\n\")\n",
    "    file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'this is bu'\n"
     ]
    }
   ],
   "source": [
    "# reading buffered text\n",
    "with open(\"test.txt\",\"rb\") as f:\n",
    "    file=io.BufferedReader(f)\n",
    "    \n",
    "    data=file.read(10) # skip charcter 10\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Red\" > Shutil</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   You might use shutil.copy() in various scenarios, such as:\n",
    "\n",
    "-   Creating backups of important files or directories.\n",
    "-   Duplicating files for processing or analysis without modifying the original data.\n",
    "-   Deploying files or resources in software deployment scripts.\n",
    "-   Moving files between different storage locations or devices.\n",
    "-   Implementing file synchronization or data migration tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is bufered write\n",
      "this is secon dline trying to write bufered write\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#open file in python open a file\n",
    "file1 = open(\"test.txt\", \"r\") # opening in read mode\n",
    "\n",
    "# read the file\n",
    "read_content = file1.read()\n",
    "print(read_content)\n",
    "\n",
    "#Closing a file will free up the resources that were tied with the file. \n",
    "# It is done using the close() method in Python. For example,\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   to remove a non-empty directory,\n",
    "-   we can use the rmtree() method inside shutil modeule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   this permanently deletes all folder and files inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# delete \"mydir\" directory and all of its contents\n",
    "#shutil.rmtree(\"test\")\n",
    "path = \"../frames\"\n",
    "#dir = \"1685304796_2572842\"\n",
    "#c_path=path+dir\n",
    "shutil.rmtree(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color =\"orange\" > make a habit of opening file using below script </font>\n",
    "-   no need to close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can perform these actions using the AWS console, the AWS SDKs, the SageMaker Python SDK, AWS CloudFormation or the AWS CLI.\n",
      "\n",
      "For batch inference with batch transform, point to your model artifacts and input data and create a batch inference job. Instead of hosting an endpoint for inference, SageMaker outputs your inferences to an Amazon S3 location of your choice.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file2 = filepath\n",
    "\n",
    "with open(file2, \"r\") as r:\n",
    "    read_content = r.read()\n",
    "    print(read_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing to files in python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   There are two things we need to remember while writing to a file.\n",
    "-   If we try to open a file that doesn't exist, a new file is created.\n",
    "-   If a file already exists, its content is erased, and new content is added to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test2.txt\", 'w') as w:\n",
    "    w.write(\"first line.\")\n",
    "    w.write(\"second line.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first line.second line.\n"
     ]
    }
   ],
   "source": [
    "with open (\"test2.txt\", 'r') as r1:\n",
    "    read_c=r1.read()\n",
    "    print(read_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Red\"> write() vs writelines() </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.write(): write a string to file\n",
    "with open (\"test2.txt\", 'w') as w1:\n",
    "    w1.write(\"Hello, world\") #Expects a single string as input.\n",
    "    # Overwrites the existing content of the file with the new string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writelines() : Writes a list of strings to the file, where each string represents a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using writelines()\n",
    "with open('test2.txt', 'w') as file:\n",
    "    lines = ['hello\\n', 'Line 2\\n', 'Line 3\\n']\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Red\"> read() vs readlines() </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Line 2\n",
      "Line 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read() : read entire content of file as a single string\n",
    "with open (\"test2.txt\", 'r') as r1:\n",
    "    read_c=r1.read()\n",
    "    print(read_c)  # content returned as single string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readlines()\n",
    "-   Reads all the lines from the file and \n",
    "-   returns them as a list of strings, where each string represents a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello\\n', 'Line 2\\n', 'Line 3\\n']\n"
     ]
    }
   ],
   "source": [
    "with open (\"test2.txt\", 'r') as r1:\n",
    "    read_c=r1.readlines()\n",
    "    print(read_c) #A list of strings where each string corresponds to a line in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Directories and files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/multi-sy-003/Documents/code/Python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "curr_dir=os.getcwd()\n",
    "print(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.listdir() \n",
    "\n",
    "-   This method takes in a path and returns a list of subdirectories and files in that path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patterns.ipynb', 'basic.ipynb', 'set.ipynb', 'oops.ipynb', 'iterator_generator.ipynb', 'functions.ipynb', 'exceptions.ipynb', 'queue.ipynb', 'file_operations.ipynb', 'test.txt', 'large_text_file.txt', 'tuple.ipynb', 'strings.ipynb', 're.ipynb', 'test2.txt', 'logging.ipynb', 'dict.ipynb', 'list.ipynb', 'test1_mar04.txt', 'new_files_mar_04.txt', 'threading.ipynb', 'date_time.ipynb']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"test2\"):\n",
    "    os.mkdir(\"test2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"purple\"> Natsort </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   natsort is a Python library that provides natural sorting of lists. \n",
    "-   Natural sorting means sorting alphanumeric strings in a way that human beings would expect.\n",
    "-   For example, when sorting a list of file names, natural sorting ensures that\n",
    "-   file names containing numbers are sorted in numerical order rather than purely lexicographical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image1.jpg', 'image2.jpg', 'image3.jpg', 'image10.jpg', 'image20.jpg']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "# List of strings to be sorted\n",
    "strings = ['image1.jpg',\"image3.jpg\",'image10.jpg', 'image2.jpg', 'image20.jpg']\n",
    "\n",
    "# Sort the list using natsort\n",
    "sorted_strings = natsorted(strings)\n",
    "\n",
    "print(sorted_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"purple\"> json operations </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   json is lightweight data interchange format ,and language-independent\n",
    "-   used for data exchange btw server and web applications.\n",
    "-   JSON is used for data exchange between different systems.\n",
    "-   It is commonly used in web APIs to send and receive structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionay\n",
    "-   Python dictionaries are used within Python programs to organize and manipulate data.\n",
    "-   No serialization or deserialization is needed when using dictionaries within a Python program, as they are a native data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON String: {\n",
      "          \"name\": \"John\",\n",
      "          \"age\": 30,\n",
      "          \"city\": \"New York\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# encoding\n",
    "\n",
    "# Example dictionary\n",
    "data = {\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 30,\n",
    "    \"city\": \"New York\"\n",
    "}\n",
    "\n",
    "# Convert Python dictionary to JSON string\n",
    "json_string = json.dumps(data, indent=10)\n",
    "print(\"JSON String:\", json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Dictionary: {'name': 'John', 'age': 30, 'city': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# decoding\n",
    "\n",
    "# Example JSON string\n",
    "json_string = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n",
    "\n",
    "# Convert JSON string to Python dictionary\n",
    "python_dict = json.loads(json_string)\n",
    "print(\"Python Dictionary:\", python_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= \"green\"> Glob </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patterns.ipynb', 'basic.ipynb', 'set.ipynb', 'oops.ipynb', 'iterator_generator.ipynb', 'functions.ipynb', 'exceptions.ipynb', 'queue.ipynb', 'file_operations.ipynb', 'test.txt', 'large_text_file.txt', 'tuple.ipynb', 'strings.ipynb', 're.ipynb', 'test2.txt', 'logging.ipynb', 'dict.ipynb', 'list.ipynb', 'test1_mar04.txt', 'new_files_mar_04.txt', 'threading.ipynb', 'date_time.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Files: ['test.txt', 'large_text_file.txt', 'test2.txt', 'test1_mar04.txt', 'new_files_mar_04.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Match all .txt files in the current directory\n",
    "txt_files = glob.glob('*.txt')\n",
    "print(\"Text Files:\", txt_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files: ['patterns.ipynb', 'basic.ipynb', 'set.ipynb', 'oops.ipynb', 'iterator_generator.ipynb', 'functions.ipynb', 'exceptions.ipynb', 'queue.ipynb', 'file_operations.ipynb', 'test.txt', 'large_text_file.txt', 'tuple.ipynb', 'strings.ipynb', 're.ipynb', 'test2.txt', 'logging.ipynb', 'dict.ipynb', 'list.ipynb', 'test1_mar04.txt', 'new_files_mar_04.txt', 'threading.ipynb', 'date_time.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Match all files in the current directory\n",
    "all_files = glob.glob('*')\n",
    "print(\"All Files:\", all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/multi-sy-003/Documents/code/videos/motion_test.mp4', '/home/multi-sy-003/Documents/code/videos/motion_output.mp4']\n"
     ]
    }
   ],
   "source": [
    "directory_path=\"/home/multi-sy-003/Documents/code/videos/\"\n",
    "jpg_files=glob.glob(directory_path + \"*.mp4\")\n",
    "print(jpg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive file matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive Text Files: ['test.txt', 'large_text_file.txt', 'test2.txt', 'test1_mar04.txt', 'new_files_mar_04.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Match all .txt files in the current directory and its subdirectories\n",
    "txt_files_recursive = glob.glob('**/*.txt', recursive=True)\n",
    "print(\"Recursive Text Files:\", txt_files_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matching directoreis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "# Match all directories in the current directory\n",
    "directories = glob.glob('*/*/')\n",
    "print(\"Directories:\", directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using character ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with Numbers: []\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Match files with names like 'file1.txt', 'file2.txt', ..., 'file9.txt'\n",
    "files_with_numbers = glob.glob('file[1-9].txt')\n",
    "print(\"Files with Numbers:\", files_with_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using wildcard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Starting with 'file': ['file_operations.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Match files that start with 'file' and have any extension\n",
    "files_starting_with_file = glob.glob('file*.*')\n",
    "print(\"Files Starting with 'file':\", files_starting_with_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
