{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color =\" orange\">1.Describe YOLO experience.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region Based CNN\n",
    "- Region-Based CNN (Convolutional Neural Network) is an object detection algorithm that uses CNNs to detect objects in images. \n",
    "-   The basic idea behind region-based CNNs is to first generate a set of candidate regions or \"region proposals\" within the image or video.\n",
    "-   Then use a CNN to classify each region as containing an object or not. The regions that are classified as containing an object are then further processed to refine the object's location and classify it into a specific class.\n",
    "-   <b> This concept is also known as Two-shot Object Detection.</b>\n",
    "\n",
    "One of the most famous examples of region-based CNN is R-CNN (Regional CNN) and its variants,\n",
    "-   Fast R-CNN\n",
    "-   Faster R-CNN\n",
    "-   Mask R-CNN\n",
    "which all have been proposed to improve the speed and accuracy of the object detection process. \n",
    "These methods use a combination of selective search or other region proposal methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Weaknesses of Region-based CNNs </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   high computational cost, inefficiency, limited generalization, and lack of context.\n",
    "-    They are computationally expensive and inefficient, making them impractical for real-time object detection on low-powered devices.\n",
    "-   Additionally, they require large labeled datasets, which can be costly and time-consuming to obtain. These factors make them less suitable for certain applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Single Shot Object Detection ( SSD) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Single-shot object detection is a type of object detection algorithm that is able to detect objects within an image or video in a single pass without the need for multiple stages or region proposals.\n",
    "\n",
    "-   Single-shot object detectors, such as YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector), use a single convolutional neural network (CNN) to directly predict the class labels and bounding boxes of objects within an image or video.\n",
    "\n",
    "- These models are trained end-to-end using a large dataset of labeled images and their associated object-bounding boxes\n",
    "\n",
    "\n",
    "-   Single-shot object detection is considered faster and more efficient than two-shot object detection methods, as it eliminates the need for multiple stages and can be run in real time on even low-powered devices. However, it may not have the same level of accuracy as the multi-stage methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> Object Detection Evaluation metrics </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean Average Precision (mAP): The precision of a model is defined as the number of true positives divided by the number of true positives plus false positives. \n",
    "\n",
    "-   The mAP metric takes into account both the precision and recall of the model and is calculated as the mean of the average precision for each class.\n",
    "-    A higher mAP value indicates a better performance of the model.\n",
    "\n",
    "2. Average Precision (AP): \n",
    "-   It is a measure of the precision of the model at different recall levels.\n",
    "-    The precision is defined as the number of true positives divided by the number of true positives plus false positives.\n",
    "-    The recall is defined as the number of true positives divided by the number of true positives plus false negatives. \n",
    "-   AP is calculated as the area under the precision-recall curve. A higher AP value indicates a better performance of the model.\n",
    "\n",
    "3. Intersection over Union (IoU):\n",
    "-    IoU is a measure of the overlap between the predicted bounding box and the ground-truth bounding box. \n",
    "-   It is calculated by taking the intersection areas of the two boxes and dividing it by the area of their union. \n",
    "-   A higher IoU value indicates a better match between the predicted and ground-truth bounding boxes.\n",
    "\n",
    " IOU = Area of overlap/ Area of union\n",
    " \n",
    "There are also several metrics that evaluate the accuracy too. For example, True Positive Rate (TPR), False Positive Rate (FPR), F1-score, and Log Average Miss Rate (MR). In addition to these metrics, object detection models can also be evaluated based on their computational efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> how yolo works </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YOLO algorithm divides the input image into a grid of cells, and for each cell, it predicts the probability of the presence of an object and the bounding box coordinates of the object.\n",
    " It also predicts the class of the object. Unlike two-stage object detectors such as R-CNN and its variants, YOLO processes the entire image in one pass, making it faster and more efficient.\n",
    "\n",
    "The basic idea behind YOLO is to divide the input image into a grid of cells and, for each cell, predict the probability of the presence of an object and the bounding box coordinates of the object. The process of YOLO can be broken down into several steps:\n",
    "\n",
    "1. Input image is passed through a CNN to extract features from the image.\n",
    "\n",
    "2. The features are then passed through a series of fully connected layers, which predict ‌class probabilities and bounding box coordinates.\n",
    "\n",
    "3. The image is divided into a grid of cells, and each cell is responsible for predicting a set of bounding boxes and class probabilities.\n",
    "\n",
    "4. The output of the network is a set of bounding boxes and class probabilities for each cell.\n",
    "\n",
    "5. The bounding boxes are then filtered using a post-processing algorithm called non-max suppression to remove overlapping boxes and choose the box with the highest probability.\n",
    "\n",
    "6. The final output is a set of predicted bounding boxes and class labels for each object in the image.\n",
    "\n",
    "One of the key advantages of YOLO is that it processes the entire image in one pass, making it faster and more efficient than two-stage object detectors such as R-CNN and its variants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> NMS (Non Max Suppression) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov2 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main differences between YOLO v2 and the original YOLO is the use of anchor boxes. In YOLO v2, ‌CNN predicts not only the bounding box coordinates but also the anchor boxes. Anchor boxes are pre-defined boxes of different aspect ratios and scales, which are used to match the predicted bounding boxes with the actual objects in the image. This allows YOLO v2 to handle objects of different shapes and sizes better.\n",
    "\n",
    "Another key difference is the use of a multi-scale approach. In YOLO v2, the input image is fed through ‌CNN at multiple scales, which allows the model to detect objects at different sizes. This is achieved by using a feature pyramid network (FPN), which allows the model to extract features at different scales from the same image.\n",
    "\n",
    "Additionally, YOLO v2 uses a different loss function than the original YOLO, called the sum-squared error (SSE) loss function. The SSE loss function is more robust and helps the model to converge faster.\n",
    "\n",
    "In terms of architecture, YOLO v2 uses a slightly deeper CNN than YOLO, which allows it to extract more powerful features from the image. The CNN is followed by several fully connected layers, which predict ‌class probabilities and bounding box coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov2 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO v2, also known as YOLO 9000, is an improved version of the original YOLO object detection algorithm. It builds upon the concepts and architecture of YOLO, but addresses some of the limitations of the original version.\n",
    "\n",
    "One of the main differences between YOLO v2 and the original YOLO is the use of anchor boxes. In YOLO v2, ‌CNN predicts not only the bounding box coordinates but also the anchor boxes. Anchor boxes are pre-defined boxes of different aspect ratios and scales, which are used to match the predicted bounding boxes with the actual objects in the image. This allows YOLO v2 to handle objects of different shapes and sizes better.\n",
    "\n",
    "Another key difference is the use of a multi-scale approach. In YOLO v2, the input image is fed through ‌CNN at multiple scales, which allows the model to detect objects at different sizes. This is achieved by using a feature pyramid network (FPN), which allows the model to extract features at different scales from the same image.\n",
    "\n",
    "Additionally, YOLO v2 uses a different loss function than the original YOLO, called the sum-squared error (SSE) loss function. The SSE loss function is more robust and helps the model to converge faster.\n",
    "\n",
    "In terms of architecture, YOLO v2 uses a slightly deeper CNN than YOLO, which allows it to extract more powerful features from the image. The CNN is followed by several fully connected layers, which predict ‌class probabilities and bounding box coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov3 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO v3 is the third version of the YOLO object detection algorithm. The first difference between YOLO v3 and previous versions is the use of multiple scales in the input image. YOLO v3 uses a technique called \"feature pyramid network\" (FPN) to extract features from the image at different scales. This allows the model to detect objects of different sizes in the image.\n",
    "\n",
    "Another important difference is the use of anchor boxes. In YOLO v3, anchor boxes are used to match the predicted bounding boxes with the actual objects in the image. Anchor boxes are pre-defined boxes of different aspect ratios and scales, and the model predicts the offset of the anchor boxes relative to the bounding boxes. This helps the model to handle objects of different shapes and sizes better.\n",
    "\n",
    "In terms of architecture, YOLO v3 is built on a deep convolutional neural network (CNN) that is composed of many layers of filters. The CNN is followed by several fully connected layers, which predict ‌class probabilities and bounding box coordinates.\n",
    "\n",
    "YOLO v3 also uses a different loss function than previous versions. It uses a combination of classification loss and localization loss, which allows the model to learn both the class probabilities and the bounding box coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov4 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s take a look at one of the most important versions of YOLO. A key distinction between YOLO v4 and previous versions is using a more advanced neural network architecture. YOLO v4 uses a technique called \"Spatial Pyramid Processing\" (SPP) to extract features from the image at different scales and resolutions. This allows the model to detect objects of different sizes in the image.\n",
    "\n",
    "Additionally, YOLO v4 also uses a technique called \"Cross-stage partial connection\" (CSP) to improve the model's accuracy. It uses a combination of multiple models with different architectures and scales and combines their predictions to achieve better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov5 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO v5 was introduced in 2020 with a key difference from the previous versions, which is the use of a more efficient neural network architecture called EfficientDet, which is based on the EfficientNet architecture. EfficientDet is a family of image classification models that have achieved state-of-the-art performance on a number of benchmark datasets. The EfficientDet architecture is designed to be efficient in terms of computation and memory usage while also achieving high accuracy.\n",
    "\n",
    "Another important difference is the use of anchor-free detection, which eliminates the need for anchor boxes used in previous versions of YOLO. Instead of anchor boxes, YOLO v5 uses a single convolutional layer to predict the bounding box coordinates directly, which allows the model to be more flexible and adaptable to different object shapes and sizes.\n",
    "\n",
    "YOLO v5 also uses a technique called \"Cross mini-batch normalization\" (CmBN) to improve the model's accuracy. CmBN is a variant of the standard batch normalization technique that is used to normalize the activations of the neural network.\n",
    "\n",
    "Regarding training, YOLO v5 uses transfer learning, which allows it to be pre-trained on a large dataset and then fine-tuned on a smaller dataset. This allows the model to learn from a wide range of data and generalize better to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov6 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notable contrast between YOLO v6 and previous versions is the use of a more efficient and lightweight neural network architecture; this allows YOLO v6 to run faster and with fewer computational resources. The architecture of YOLO v6 is based on the \"EfficientNet-Lite\" family, which is a set of lightweight models that can be run on various devices with limited computational resources.\n",
    "\n",
    "YOLO v6 also incorporates data augmentation techniques to improve the robustness and generalization of the model. This is done by applying random transformations to the input images during training, such as rotation, scaling, and flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> yolov7 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO v7, on the most recent stable iterations of the YOLO algorithm. It boasts a number of enhancements compared to ‌previous versions. A key enhancement is the implementation of anchor boxes. These anchor boxes, which come in various aspect ratios, are utilized to identify objects of various shapes. The use of nine anchor boxes in YOLO v7 enables it to detect a wider range of object shapes and sizes, leading to a decrease in false positives.\n",
    "\n",
    "In YOLO v7, a new loss function called \"focal loss\" is implemented to enhance performance. Unlike the standard cross-entropy loss function used in previous versions of YOLO, focal loss addresses the difficulty in detecting small objects by adjusting the weight of the loss on well-classified examples and placing more emphasis on challenging examples to detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> What are Pretrained weights? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9. What is skip connections?\n",
    "10. Explain - accuracy, precision and recall\n",
    "11. How to apply these metrics on multi-class classification\n",
    "12. DIfference between gray-scale image and b/w image\n",
    "13. Why Adam works better than SGD\n",
    "\n",
    "15. What are image filters?\n",
    "\n",
    "19. What is noise in image?\n",
    "20. When to you use Sigmoid and Softmax\n",
    "21. Difference between pytorch and tensorflow and keras\n",
    "22. Explain BERT\n",
    "23. Difference between BERT and transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import pretrain model\n",
    "4. How to remove html tags from python\n",
    "5. Write sequence model in tensorflow\n",
    "6. How good r u at python ? what applications u have developed ? generally what libraries\n",
    "u use in python?\n",
    "7. What is decorator, generator, coroutine, dataclass, metaclass, context variables, global\n",
    "variables in python?\n",
    "8. Explain async await\n",
    "9. Difference between git and github\n",
    "10. Explain SDLC - waterfall and agile methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is nltk?\n",
    "2. How u use nltk for summarisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative AI questions\n",
    "1. What is GEN AI\n",
    "2. What is LLM\n",
    "3.  Explain your use case of LLM\n",
    "4. Explain RAG Architecture\n",
    "5. What exactly retreival is doing?\n",
    "6. Explain RAG pipeline with example.\n",
    "7. RAG use cases\n",
    "8. Libraries and framework use in RAG\n",
    "9. What models u use FOR rag chatbot?\n",
    "10. Have u use openai models for rag?\n",
    "3. Explain Chunking\n",
    "4. What is overlapping in chunking\n",
    "5. Explain your use case implemented using RAG\n",
    "6. What is Vector DB\n",
    "7. Which Vector DB used?\n",
    "8. Explain the difference between Nosql vs VectorDB\n",
    "9. Explain semantic vs similarity score\n",
    "10. How to do text processing for chatbot where your data is stored in pdf\n",
    "11. What is finetuning in genai?\n",
    "12. Explain Lora\n",
    "13. Frameworks and libraries you r using for Gen ai , nlp , python\n",
    "14. Explain your projects use cases for Gen AI\n",
    "15. How you monitor your model?\n",
    "16. What are the testing criteria?\n",
    "17. Any of your nlp application that you deployed on AWS?\n",
    "18. What is embedding\n",
    "19. Which libraries u use for embedding\n",
    "20. What's the difference between using openai embedding and sentence transformer\n",
    "embedding?\n",
    "21. Which vector database u use?\n",
    "22. Various options available for vector database\n",
    "23. Frameworks for Gen ai\n",
    "24. Is your layoutllm3 finetuned or pretain prediction?\n",
    "25. How u use Genai to generate something in your project?\n",
    "29. suppose you created llm howwill you manage bulk of user requests\n",
    "30. fine tuning llm using lora,qlore\n",
    "31. model quantization in llm\n",
    "31. why we should not use normal db than vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Bagging and boosting\n",
    "-  CNN architecture\n",
    "-  bias varianve tradeoff\n",
    "-  loss functions in logistic regression\n",
    "-  drawbacks of traditional RNN\n",
    "-  lstm architecture\n",
    "-  fine tuning llm techniques\n",
    "-  chain rule of action\n",
    "-  under fitting adn overfitting\n",
    "-  loss fun of yolos\n",
    "-  logistic regression\n",
    "-  encoder-decoder architecture\n",
    "- word embedding techniques\n",
    "- differece between encoder and decoder\n",
    "- reverse string without inbuilt function\n",
    "- find max in an array without inbuilt function\n",
    "- remove html tag from paragraph\n",
    "- what is image filters\n",
    "- what is kernal\n",
    "- what is pixel\n",
    "- how ann and cnn works\n",
    "- what is noise in image\n",
    "- explain loss function of yolo\n",
    "- write python code to generate array\n",
    "- what is Decison Tree? explain ID3 and CART? which one you choose\n",
    "- metric used for object det\n",
    "- Explain about IOU\n",
    "_ explain NMS in yolo\n",
    "- scenario based NMS questions\n",
    "- how skip connection works\n",
    "- what causes vanishing gradient and how to overcome it\n",
    "- what are learnable parameters in model training\n",
    "- what happens if we apply dropout in network\n",
    "- why cant we use mean sq error in classfication problem\n",
    "- for small object detection which one method should use -morphological erosion or bilateral filter?\n",
    "- difference between HOG and YOLO\n",
    "- difference between yolov5 and yolv8 model\n",
    "- explain loss functions of yolo # https://pub.towardsai.net/interview-questions-object-detection-9430d7dee763\n",
    "- how you deployed model in edge devices\n",
    "- what is learning rate\n",
    "- https://www.geeksforgeeks.org/deep-learning-interview-questions/\n",
    "- https://www.geeksforgeeks.org/machine-learning-interview-questions/\n",
    "- explaina architecture of vgg16 and vgg19 \n",
    "- what do you meman by fine tuning in image classfication\n",
    "- diff bet cnn and ann\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What are the advantages of neural networks?\n",
    "Following are the advantages of neural networks:\n",
    "Neural networks are extremely adaptable, and they may be used for both\n",
    "classification and regression problems, as well as much more complex\n",
    "problems. Neural networks are also quite scalable. We can create as many layers\n",
    "as we wish, each with its own set of neurons. When there are a lot of data points,\n",
    "neural networks have been shown to generate the best outcomes. They are best\n",
    "used with non-linear data such as images, text, and so on. They can be applied\n",
    "to any data that can be transformed into a numerical value.\n",
    "Once the neural network mode has been trained, they deliver output very fast.\n",
    "Thus, they are time-eﬀective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage of neural ntw\n",
    "The \"black box\" aspect of neural networks is a well-known disadvantage. That is,\n",
    "we have no idea how or why our neural network produced a certain result. When\n",
    "we enter a dog image into a neural network and it predicts that it is a duck, we\n",
    "may find it challenging to understand what prompted it to make this prediction.\n",
    "It takes a long time to create a neural network model.\n",
    "Neural networks models are computationally expensive to build because a lot of\n",
    "computations need to be done at each layer.\n",
    "A neural network model requires significantly more data than a traditional\n",
    "machine learning model to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you understand about gradient clipping in the\n",
    "context of deep learning?\n",
    "Gradient Clipping is a technique for dealing with the problem of exploding gradients\n",
    "(a situation in which huge error gradients build up over time, resulting in massive\n",
    "modifications to neural network model weights during training) that happens during\n",
    "backpropagation. The problem of exploding gradients occurs when the gradients get\n",
    "excessively big during training, causing the model to become unstable. If the gradient\n",
    "has crossed the anticipated range, the gradient values are driven element-by-\n",
    "element to a specific minimum or maximum value. Gradient clipping improves\n",
    "numerical stability while training a neural network, but it has little eﬀect on the\n",
    "performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain Data Normalisation. What is the need for it?\n",
    "Data Normalisation is a technique in which data is transformed in such a way that\n",
    "they are either dimensionless or have a similar distribution. It is also known as\n",
    "standardization and feature scaling. It's a pre-processing procedure for the input\n",
    "data that removes redundant data from the dataset.\n",
    "Normalization provides each variable equal weights/importance, ensuring that no\n",
    "single variable biases model performance in its favour simply because it is larger. It\n",
    "vastly improves model precision by converting the values of numeric columns in a\n",
    "dataset to a similar scale without distorting the range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the diﬀerent techniques to achieve data\n",
    "normalization?\n",
    "Rescaling: Rescaling data is the process of multiplying each member of a data\n",
    "set by a constant term k, or changing each integer x to f(X), where f(x) = kx and k\n",
    "and x are both real values. The simplest of all approaches, rescaling (also known\n",
    "as \"min-max normalization\"), is calculated as:\n",
    "{\"detectHand\":false}\n",
    "This represents the rescaling factor for every data point x.\n",
    "\n",
    "\n",
    "\n",
    "Mean Normalisation: In the transformation process, this approach employs the\n",
    "mean of the observations:\n",
    "{\"detectHand\":false}\n",
    "This represents the mean normalizing factor for every data point x.\n",
    "\n",
    "\n",
    "\n",
    "Z-score Normalisation: This technique, also known as standardization,\n",
    "employs the Z-score or \"standard score.\" SVM and logistic regression are two\n",
    "examples of machine learning algorithms that utilise it:\n",
    "{\"detectHand\":false}\n",
    "This represents the Z-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diﬀerence between multi-class and multi-label\n",
    "classification problems.\n",
    "\n",
    "The classification task in a multi-class classification problem has more than two\n",
    "mutually exclusive classes (classes that have no intersection or no attributes in\n",
    "common), whereas in a multi-label classification problem, each label has a diﬀerent\n",
    "classification task, although the tasks are related in some way. For example,\n",
    "classifying a group of photographs of animals that could be cats, dogs, or bears is a\n",
    "multi-class classification problem that assumes each sample can be of only one type,\n",
    "implying that an image can be categorized as either a cat or a dog, but not both at\n",
    "the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to train a neural network model by setting all\n",
    "biases to 0? Also, is it possible to train a neural network\n",
    "model by setting all of the weights to 0?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Yes, even if all of the biases are set to zero, the neural network model has a chance of\n",
    "learning.\n",
    "No, training a model by setting all of the weights to 0 is impossible since the neural\n",
    "network will never learn to complete a task. When all weights are set to zero, the\n",
    "derivatives for each w remain constant, resulting in neurons learning the same\n",
    "features in each iteration. Any constant initialization of weights, not simply zero, is\n",
    "likely to generate a poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a tensor in deep learning?\n",
    "A tensor is a multidimensional array that represents a generalization of vectors and\n",
    "matrices. It is one of the key data structures used in deep learning. Tensors are\n",
    "represented as n-dimensional arrays of base data types. The data type of each\n",
    "element in the Tensor is the same, and the data type is always known. It's possible\n",
    "that only a portion of the shape (that is, the number of dimensions and the size of\n",
    "each dimension) is known. Most operations yield fully-known tensors if their inputs\n",
    "are likewise fully known, however, in other circumstances, the shape of a tensor can\n",
    "only be determined at graph execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the diﬀerence between a shallow network and a\n",
    "deep network.\n",
    "A hidden layer, as well as input and output layers, are present in every neural\n",
    "network. Shallow neural networks are those that have only one hidden layer,\n",
    "whereas deep neural networks include numerous hidden layers. Both shallow and\n",
    "deep networks can fit into any function, however, shallow networks require a large\n",
    "number of input parameters, whereas deep networks, because of their several layers,\n",
    "can fit functions with a small number of input parameters. Deep networks are\n",
    "currently favored over shallow networks because the model learns a new and\n",
    "abstract representation of the input at each layer. In comparison to shallow\n",
    "networks, they are also far more eﬀicient in terms of the number of parameters and\n",
    "computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain Stochastic Gradient Descent. How is it diﬀerent\n",
    "from Batch Gradient Descent?\n",
    "Stochastic Gradient Descent: Stochastic Gradient Descent seeks to tackle the major\n",
    "diﬀiculty with Batch Gradient Descent, which is the use of the entire training set to\n",
    "calculate gradients at each step. It is stochastic in nature, which means it chooses up\n",
    "a \"random\" instance of training data at each step and then computes the gradient,\n",
    "which is significantly faster than Batch Gradient Descent because there are much\n",
    "fewer data to modify at once. Stochastic Gradient Descent is best suited for\n",
    "unconstrained optimization problems. The stochastic nature of SGD has a drawback\n",
    "in that once it gets close to the minimum value, it doesn't settle down and instead\n",
    "bounces around, giving us a good but not optimal value for model parameters. This\n",
    "can be solved by lowering the learning rate at each step, which will reduce the\n",
    "bouncing and allow SGD to settle down at the global minimum a er some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "\n",
    "The gradient is\n",
    "calculated using the\n",
    "entire training dataset.\n",
    "\n",
    "\n",
    "It is slow and\n",
    "computationally more\n",
    "expensive than\n",
    "Stochastic Gradient\n",
    "Descent.\n",
    "\n",
    "\n",
    "It is not recommended\n",
    "for large training\n",
    "samples.\n",
    "\n",
    "\n",
    "Given enough time to\n",
    "converge, it returns the\n",
    "best answer.\n",
    "\n",
    "\n",
    "There is no need to\n",
    "shuﬀle the data points at\n",
    "random\n",
    "\n",
    "\n",
    "In this, the convergence\n",
    "is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "A single training sample is used\n",
    "to compute the gradient.\n",
    "\n",
    "It is faster and less\n",
    "computationally expensive than\n",
    "Batch Gradient Descent.\n",
    "\n",
    "It is recommended for large\n",
    "training samples.\n",
    "\n",
    "It provides a good solution, but\n",
    "not the best.\n",
    "\n",
    "Because we want the data\n",
    "sample to be in a random order,\n",
    "we'll shuﬀle the training set for\n",
    "each epoch.\n",
    "\n",
    "It arrives at the convergence\n",
    "point substantially faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an activation function? What is the use of an\n",
    "activation function?\n",
    "An artificial neural network's activation function is a function that is introduced to\n",
    "help the network learn complex patterns in the data. When compared to a neuron-\n",
    "based model seen in our brains, the activation function is responsible for\n",
    "determining what is to be fired to the next neuron at the end of the process. In an\n",
    "ANN, an activation function performs the same job. It takes the preceding cell's\n",
    "output signal and turns it into a format that may be used as input to the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, x0 and x1 are the inputs. w1 is the weight and a is the activation function.\n",
    "The activation function introduces non-linearity into the neural network, allowing it\n",
    "to learn more complex functions. The neural network would only be able to learn a\n",
    "function that is a linear combination of its input data if it didn't have the Activation\n",
    "function.\n",
    "The activation function converts inputs to outputs. The activation function is in\n",
    "charge of determining whether or not a neuron should be stimulated. It arrives at a\n",
    "decision by calculating the weighted total and then adds bias. The activation\n",
    "function's main goal is to introduce non-linearity into a neuron's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you mean by an epochs in the context of deep\n",
    "learning?\n",
    "An epoch is a terminology used in deep learning that refers to the number of passes\n",
    "the deep learning algorithm has made across the full training dataset. Batches are\n",
    "commonly used to group data sets (especially when the amount of data is very large).\n",
    "The term \"iteration\" refers to the process of running one batch through the model.\n",
    "The number of epochs equals the number of iterations if the batch size is the entire\n",
    "training dataset. This is frequently not the case for practical reasons. Several epochs\n",
    "are used in the creation of many models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building a neural network architecture, how will you\n",
    "decide how many neurons and the hidden layers should the\n",
    "neural network have?\n",
    "There is no clear and fast rule for determining the exact number of neurons and\n",
    "hidden layers required to design a neural network architecture given a business\n",
    "problem. The size of the hidden layer in a neural network should be somewhere\n",
    "between the size of the output layers and that of the input layers. However, there are\n",
    "a few basic ways that might help you get a head start on constructing a neural\n",
    "network architecture:\n",
    "The best method to approach any unique real-world predictive modelling\n",
    "problem is to start with some basic systematic experimentation to see what\n",
    "would perform best for any given dataset based on previous experience working\n",
    "with neural networks in similar real-world situations. The network configuration\n",
    "can be chosen based on one's understanding of the problem domain and\n",
    "previous expertise with neural networks. The number of layers and neurons\n",
    "employed on similar issues is always a good place to start when evaluating a\n",
    "neural network's configuration.\n",
    "It is best to start with simple neural network architecture and gradually increase\n",
    "the complexity of the neural network based on predicted output and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a deep learning model be solely built on linear\n",
    "regression?\n",
    "Yes, if the problem is represented by a linear equation, deep networks can be built\n",
    "using a linear function as the activation function for each layer. A problem that is a\n",
    "composition of linear functions, on the other hand, is a linear function, and there is\n",
    "nothing spectacular that can be accomplished by implementing a deep network\n",
    "because adding more nodes to the network will not boost the machine learning\n",
    "model's predictive capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to you, which one is more powerful - a two layer\n",
    "neural network without any activation function or a two\n",
    "layer decision tree?\n",
    "A two-layer neural network is made up of three layers: one input layer, one hidden\n",
    "layer, and one output layer. When dealing with neural networks, an activation\n",
    "function is essential since it is required when dealing with complex and nonlinear\n",
    "functional mappings between inputs and response variables. When there is no\n",
    "activation function in a two-layer neural network, it is simply a linear network. A\n",
    "Neural Network without an Activation function is just a Linear Regression Model,\n",
    "which has limited capability and frequently fails to perform well.\n",
    "A decision tree with a depth of two layers is known as a two-layer decision tree.\n",
    "Decision Trees are a type of supervised machine learning (that is, the machine is fed\n",
    "with what the input is and what the related output is in the training data) in which\n",
    "the data is continually split according to a parameter. Two entities, decision nodes,\n",
    "and leaves can be used to explain the tree. The decisions or final outcomes are\n",
    "represented by the leaves. And the data is separated at the decision nodes.\n",
    "When comparing these two models, the two-layer neural network (without activation\n",
    "function) is more powerful than the two-layer decision tree, because the two-layer\n",
    "neural network will consider more attributes while building a model, whereas the\n",
    "two-layer decision tree will only consider 2 or 3 attributes.\n",
    "Page 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diﬀerentiate between bias and variance in the context of\n",
    "deep learning models. How can you achieve balance\n",
    "between the two?\n",
    "Comprehending prediction errors is crucial when it comes to understanding\n",
    "predictions. Reducible (errors that arise due to squared bias or squared variance) and\n",
    "irreducible (errors that arise due to the randomness or natural variability in a system\n",
    "and cannot be reduced by varying the model) mistakes are the two primary types of\n",
    "errors. There are two types of reducible errors: bias and variance. Gaining a thorough\n",
    "grasp of these flaws aids in the construction of an accurate model by preventing\n",
    "overfitting and underfitting.\n",
    "Bias:\n",
    "The bias is defined as the diﬀerence between the ML model's predicted values and\n",
    "the actual value. Biasing results in a substantial inaccuracy in both training and\n",
    "testing data. To avoid the p\n",
    "\n",
    "The data predicted is in a straight line format due to significant bias, and hence does\n",
    "not fit accurately in the data set. Underfitting of data is the term for this type of\n",
    "fitting. This occurs when the theory is too straightforward or linear. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variance:\n",
    "The variance of the model is the variability of model prediction for a given data point,\n",
    "which tells us about the dispersion of our data. It is the diﬀerence between the\n",
    "validation error and the training error. The model with high variance has a very\n",
    "complex fit to the training data and so is unable to fit accurately on new data. As a\n",
    "result, while such models perform well on training data, they have high error rates\n",
    "when testing data.\n",
    "When a model's variance is excessive, it's referred to as Overfitting of Data.\n",
    "Overfitting, which involves accurately fitting the training set using a complicated\n",
    "curve and a high order hypothesis, is not a viable option because the error with\n",
    "unknown data is considerable.\n",
    "Variance should be kept to a minimum when training a data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model must always aim for a low bias and a low variance in order to achieve the\n",
    "best balance between the two mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What exactly do you mean by exploding and vanishing\n",
    "gradients?\n",
    "By taking incremental steps towards the minimal value, the gradient descent\n",
    "algorithm aims to minimize the error. The weights and biases in a neural network are\n",
    "updated using these processes.\n",
    "However, at times, the steps grow excessively large, resulting in increased updates to\n",
    "weights and bias terms — to the point where the weights overflow (or become NaN,\n",
    "that is, Not a Number). An exploding gradient is the result of this, and it is an unstable\n",
    "method.\n",
    "On the other hand, if the steps are excessively small, it results in minor – even\n",
    "negligible – changes in the weights and bias terms. As a result, we may end up\n",
    "training a deep learning model with nearly identical weights and biases every time,\n",
    "never reaching the least error function. The vanishing gradient is what it's called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization approach that helps to avoid overfitting and hence\n",
    "improves generalizability (that is, the model predicts correct output for most of the\n",
    "inputs in general, rather than only being limited to the training data set). In general,\n",
    "we should utilize a low dropout value of 20 percent to 50 percent of neurons, with\n",
    "20% being a decent starting point. A probability that is too low has no eﬀect, whereas\n",
    "a number that is too high causes the network to under-learn.\n",
    "When you employ dropout on a larger network, you're more likely to achieve better\n",
    "results because the model has more opportunities to learn independent\n",
    "representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">HOG, or Histogram of Oriented Gradients </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A histogram is an approximate representation of the distribution of numerical data that looks like a looks a bar graph\n",
    "- fwture extraction algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Trasformer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n summary, self-attention allows a transformer model to attend to different parts of the same input sequence, while attention allows a transformer model to attend to different parts of another sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " self-attention refers to the the same sequence which is currently being encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input = \" I love python\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _01 : convert sentence to tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_02: word embedding : embedding is representation of words in dense vector\n",
    "-    using word2vec or glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_03 : Positional encoding\n",
    "-    we do positional encoding so that w ecan pass postion of each word in a sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _04 : Encoder\n",
    "-   self attention :\n",
    "-       capture the dependencies between different words in the input sequence and learn to focus on the most relevant words for each position.\n",
    "-    each word in sentence gets tranformed into query ,key, value\n",
    "\n",
    "Attention calculation:\n",
    "-   similarity score calculated btw two two words\n",
    "-   each wors query and all other words key.....the higher the score the more siumlarity,words with higher score got high attention\n",
    "\n",
    "Attention weights calcuated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder use  self attention to understand output sequence which it will generating and waiting\n",
    "decoder genertes output sequence one word at a time, so masking is important in order to prevent decoder to genrates future words, means it should generates only single word which attains to previous context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Transformer architecture has multi-head attention which means that multiple sets (or heads) of self-attention are learnt in parallel. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Vision \n",
    "Trasformer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self attention in vision trnasformwr allow model to understand relation between different parts of image by assigning score to different patches and focusing on most relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _01 : break images into patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step : 02 \n",
    "-   patch size and stride used in original paper is 16\n",
    "-    if a image of shape 224*224\n",
    "-   then patches would be (224/16)*(224/16)=194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step : 03\n",
    "-   flatten the patches from 2D to 1D\n",
    "-   each patch is flattend into !D vector\n",
    "-   convert patch to sequence of tokens like patch of tree(16*16) : get converted to 256 token\n",
    "-    these tokens are passed to transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> vision trnasformer only have ecoder</font\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in encoder 1D falttnd vector are converted into low dimensional vector using linear projections in order to maintain relationship between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear projection has two steps\n",
    "-   weight matrix multiplicaton\n",
    "-   bias addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After liner projection we apply positional embeddingthen we pass that embedding to self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "therw is one more layer in encoder in case of vision transformer ['MLP'] head which help to provide probabilites of evry class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Bert: Bidirectional Encoder Representation from Trnasformer </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
