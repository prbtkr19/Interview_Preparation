{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the challenges/problems that RNNs have which was solved using Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long-Term Dependencies:\n",
    "\n",
    "-   RNNs suffer from difficulties in capturing long-term dependencies in sequences.\n",
    "-    As the sequence length increases, the influence of earlier elements diminishes, leading to vanishing or exploding gradients. \n",
    "\n",
    "-   Transformers alleviate this issue by using self-attention mechanisms, allowing direct connections between any two positions in the sequence. \n",
    "-   This enables transformers to capture long-range dependencies more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization:\n",
    "\n",
    "-   RNNs process sequences sequentially, which limits parallelization and slows down training. \n",
    "\n",
    "-   Transformers, on the other hand, can process all elements in the sequence simultaneously since the attention mechanism operates in parallel.\n",
    "-    This property allows transformers to take advantage of parallel computing architectures and significantly speed up training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Understanding: \n",
    "\n",
    "-   RNNs struggle to retain context information from earlier elements in the sequence, which can impact their ability to understand and generate coherent text. \n",
    "\n",
    "-   Transformers employ self-attention to model dependencies across the entire sequence, allowing each position to attend to all other positions.\n",
    "-   This enables transformers to have a global view of the context and capture contextual relationships more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding: \n",
    "\n",
    "-   RNNs inherently encode sequential information through their recurrent connections. \n",
    "-   In contrast, transformers lack explicit sequential connections. \n",
    "\n",
    "-   To overcome this limitation, transformers incorporate positional encoding, which introduces information about the position/order of elements in the sequence. \n",
    "-   This positional encoding allows the model to capture the sequential order without relying on recurrent connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is Word Embedding?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Word embedding refers to the representation of words or phrases as dense vectors in a high-dimensional vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings have several benefits in NLP tasks:\n",
    "\n",
    "-   Semantic Similarity:\n",
    "\n",
    "     Word embeddings can capture semantic relationships, allowing for measuring similarity between words and finding related words. This enables tasks like finding synonyms or identifying words with similar meanings.\n",
    "\n",
    "-   Contextual Understanding:\n",
    "\n",
    "     Word embeddings can capture contextual information by considering the words that appear around a target word. This allows the embeddings to capture nuances in word meanings based on their specific contexts.\n",
    "\n",
    "-   Dimension Reduction: \n",
    "    \n",
    "    Word embeddings provide a more compact representation of words compared to one-hot vectors. By using lower-dimensional dense vectors, they can effectively capture important features of words while reducing computational complexity.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Popular word embedding models include Word2Vec, GloVe (Global Vectors for Word Representation), and FastText. These models are trained on large text corpora and generate </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is Encoder Decoder Architecture?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The encoder-decoder architecture is a framework commonly used in sequence-to-sequence models, where an input sequence is transformed into an output sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The encoder component processes the input sequence and generates a fixed-dimensional representation, often called the context vector or the encoded representation.\n",
    "-    It aims to capture the relevant information and context from the input sequence. \n",
    "-   The encoder can be built using various neural network architectures, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformer models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "-   The decoder component takes the encoded representation from the encoder and uses it as a starting point to generate the output sequence. \n",
    "-   It receives the encoded representation as an input and generates the output tokens step by step. At each step, the decoder predicts the next token based on the previously generated tokens and the encoded representation.\n",
    "-    The decoder can also employ recurrent networks or transformer models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> The encoder-decoder architecture is widely used in tasks such as machine translation, text summarization, speech recognition, and image captioning. Here’s a high-level overview of the typical workflow of an encoder-decoder model: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is attention?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In transformers, “attention” refers to a mechanism that allows the model to focus on different parts of the input sequence when processing it. \n",
    "\n",
    "-   It enables the model to assign varying degrees of importance or relevance to different elements within the sequence.\n",
    "\n",
    "-   The attention mechanism in transformers is known as “self-attention” or “scaled dot-product attention.” \n",
    "-   It operates on a sequence of vectors, such as words in a sentence or pixels in an image, and computes a weighted sum of these vectors to obtain context-aware representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> The attention mechanism consists of three key components:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Query, Key, and Value: \n",
    "\n",
    "-   The input sequence is transformed into three sets of vectors: queries, keys, and values. \n",
    "-   These vectors are linearly projected to lower-dimensional representations to capture the relationships between different elements in the sequence.\n",
    "\n",
    "2. Attention Scores:\n",
    "\n",
    "-    For each query, attention scores are computed by taking the dot product between the query and each key.\n",
    "-    These scores represent the relevance or similarity between the query and each key and determine the importance of each key’s value for the given query.\n",
    "\n",
    "3. Weighted Sum: \n",
    "\n",
    "-   The attention scores are then normalized using a softmax function to obtain attention weights. \n",
    "-   These weights determine how much attention or importance should be assigned to each value. The values are multiplied by their corresponding attention weights and summed to produce the final attention output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> There are different types of attention mechanisms commonly used in neural networks, including: </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product Attention: \n",
    "\n",
    "-   This is one of the simplest forms of attention. It computes the dot product between a query vector and a set of key vectors to obtain the attention scores. The attention scores are then used to weight the values, which are combined to form the final output.\n",
    "\n",
    "Scaled Dot Product Attention:\n",
    "\n",
    "-    This type of attention is similar to dot product attention, but it incorporates a scaling factor to prevent the dot product from becoming too large. The scaling factor is typically the square root of the dimension of the key vectors.\n",
    "\n",
    "Additive Attention:\n",
    "\n",
    "-    Additive attention calculates attention scores by applying a feed-forward neural network to the concatenation of the query and key vectors. The resulting scores are then used to weight the values.\n",
    "\n",
    "Multiplicative Attention:\n",
    "\n",
    "-    Multiplicative attention, also known as Bahdanau attention or concat attention, concatenates the query and key vectors and applies a non-linear transformation (such as a feed-forward neural network) followed by a dot product with a weight vector to obtain attention scores.\n",
    "\n",
    "Self-Attention: \n",
    "\n",
    "-   Self-attention, also known as scaled dot product attention, is a type of attention mechanism used in transformer models. It allows the model to attend to different positions within its own input sequence. Self-attention computes attention scores by taking the dot product of query, key, and value vectors derived from the same input sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is Positional Encoding? How is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Positional encoding in transformers is a technique used to incorporate the positional information of tokens into the input embeddings. It allows the transformer model to understand the sequential order of the tokens without relying on recurrent connections. Positional encoding helps capture the notion of relative and absolute positions within the input sequence.\n",
    "\n",
    "-   In transformers, positional encoding is calculated using sinusoidal functions. The positional encodings are added to the input embeddings, allowing the model to learn positional relationships along with the token semantics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a step-by-step overview of how positional encoding is calculated in transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Positional Encoding Matrix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   For each position in the input sequence, a positional encoding vector is created.\n",
    "\n",
    "- The positional encoding matrix has dimensions of (sequence_length, hidden_size), where sequence_length represents the length of the input sequence, and hidden_size represents the dimensionality of the input embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encoding Calculation:\n",
    "\n",
    "- Each positional encoding vector is calculated based on the position and the dimension of the embedding.\n",
    "\n",
    "- Each element of the positional encoding vector is determined by a sinusoidal function with different frequencies.\n",
    "\n",
    "- The encoding for position i and dimension j can be calculated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combining with Input Embeddings:\n",
    "\n",
    "- The positional encoding vectors are added to the input embeddings element-wise.\n",
    "\n",
    "- This is done to incorporate the positional information into the input representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sinusoidal encoding allows the model to differentiate between tokens based on their positions. The frequency of the sinusoidal functions ensures that the positional encodings have distinct values for different positions and dimensions, helping the model to capture relative positions effectively.\n",
    "\n",
    "It’s important to note that the positional encoding is added only once and remains static throughout the transformer model. It does not change during the training process.\n",
    "\n",
    "By combining the positional encoding with the token embeddings, transformers can leverage both the token semantics and the sequential order of the input sequence, enabling them to handle variable-length sequences and capture long-range dependencies effectively.\n",
    "\n",
    "Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
