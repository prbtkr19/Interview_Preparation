{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> Image Similarity Approaches </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram based approach\n",
    "-   Histograms capture the distribution of pixel values in an image.\n",
    "-   By comparing the histograms of two images, you can measure their similarity.\n",
    "- Below two metrics used for measuring similarity btw images\n",
    "   - ['The Histogram Intersection'] \n",
    "   - Histogram Correlation metrics \n",
    "\n",
    "\n",
    "-  We use grayscale histograms for thresholding. \n",
    "-  We use histograms for white balancing. \n",
    "-  We use color histograms for object tracking in images, such as with the CamShift algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">cv2.calcHist(images, channels, mask, histSize, ranges) <font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   channels: A list of indexes, where we specify the index of the channel we want to compute a          histogram for.\n",
    "\n",
    "-    To compute a histogram of a grayscale image, the list would be [0]. \n",
    "\n",
    "-   To compute a histogram for all three red, green, and blue channels, the channels list would be [0, 1, 2].\n",
    "\n",
    "-   If a mask is provided, a histogram will be computed for masked pixels only. If we do not have a mask or do not want to apply one, we can just provide a value of None.\n",
    "\n",
    "-   histSize: This is the number of bins we want to use when computing a histogram. Again, this is a list, one for each channel we are computing a histogram for. \n",
    "\n",
    "-   The bin sizes do not all have to be the same. Here is an example of 32 bins for each channel:\n",
    "\n",
    "-   ranges: The range of possible pixel values. Normally, this is [0, 256] (that is not a typo — the ending range of the cv2.calcHist function is non-inclusive so you’ll want to provide a value of 256 rather than 255) for each channel, but if you are using a color space other than RGB [such as HSV], the ranges might be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Load images\n",
    "image1 = cv2.imread(\"test1.png\")\n",
    "image2 = cv2.imread(\"test2.png\")\n",
    "\n",
    "# image2 = cv2.imread(\"test3.png\")\n",
    "# new_width = 768\n",
    "# new_height = 1366\n",
    "# image2=cv2.resize(image2,(new_width,new_height))\n",
    "\n",
    "\n",
    "\n",
    "hist_img1 = cv2.calcHist([image1], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img1[255, 255, 255] = 0 #ignore all white pixels\n",
    "cv2.normalize(hist_img1, hist_img1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "hist_img2 = cv2.calcHist([image2], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img2[255, 255, 255] = 0  #ignore all white pixels\n",
    "cv2.normalize(hist_img2, hist_img2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "# Find the metric value\n",
    "metric_val = cv2.compareHist(hist_img1, hist_img2, cv2.HISTCMP_CORREL)\n",
    "print(f\"Similarity Score: \", round(metric_val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> Structural Similarity Index (SSIM) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   SSIM is a widely used metric that assesses the structural similarity between two images.\n",
    "\n",
    "-   It considers luminance, contrast, and structure, giving a score between -1 (dissimilar) and 1 (identical).\n",
    "-    The scikit-image library in Python offers an SSIM implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1366, 3) (768, 1366, 3)\n",
      "SSIM Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import metrics\n",
    "\n",
    "# Load images\n",
    "image1 = cv2.imread(\"test1.png\")\n",
    "image2 = cv2.imread(\"test2.png\")\n",
    "\n",
    "image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]), interpolation = cv2.INTER_AREA)\n",
    "print(image1.shape, image2.shape)\n",
    "\n",
    "# Convert images to grayscale\n",
    "image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Calculate SSIM\n",
    "ssim_score = metrics.structural_similarity(image1_gray, image2_gray, full=True)\n",
    "print(f\"SSIM Score: \", round(ssim_score[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color =\"orange\"> Drawbacks of  (SSIM) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The main drawback of the SSIM approach compared to the Histogram approach is that the images have to be the same dimension.\n",
    "\n",
    "-    Even the similarity score is very low. We can do background subtraction and transparency removal from the images to improve the similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> feature based approach </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   These methods extract salient features from images, such as edges, corners, or key points. \n",
    "\n",
    "-   Techniques like Scale-Invariant Feature Transform (SIFT) and Speeded-Up Robust Features (SURF) identify distinctive points in images, which can then be compared across images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Scale-Invariant Feature Transform (SIFT)\n",
    "-   https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Speeded-Up Robust Features (SURF)\n",
    "-   https://medium.com/@deepanshut041/introduction-to-surf-speeded-up-robust-features-c7396d6e7c4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURF IMplementation (Speeded-UP Robust Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURF Composed of two steps\n",
    "-   Feature Extraction\n",
    "-   Feature Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Deep learning based approach </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep learning has revolutionized image similarity tasks. \n",
    "\n",
    "Using pre-trained convolutional neural networks (CNNs) like ResNet, VGG, and Inception, you can extract deep features from images.\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-Training) from the openAI is an impressive multimodal zero-shot image classifier that achieves impressive results in a wide range of domains with no fine-tuning. It applies the recent advancements in large-scale transformers like GPT-3 to the vision arena.\n",
    "\n",
    "We can fine-tune these models on our own image and text data with the regular SentenceTransformers training code. ScrapeHero helps in preparing your own image dataset to train these models. Its web crawling service can crawl complex websites and provides high-quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@jeremy-k/unlocking-openai-clip-part-2-image-similarity-bf0224ab5bb0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git\n",
    "#pip install open_clip_torch\n",
    "#pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@473.268] global loadsave.cpp:248 findDecoder imread_('blank.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@473.268] global loadsave.cpp:248 findDecoder imread_('add_text.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__array_interface__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(cos_scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(\u001b[43mgenerateScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mgenerateScore\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     18\u001b[0m image2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_text.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#test_img = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#data_img = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mimageEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m img2 \u001b[38;5;241m=\u001b[39m imageEncoder(image2)\n\u001b[1;32m     23\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mpytorch_cos_sim(img1, img2)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mimageEncoder\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimageEncoder\u001b[39m(img):\n\u001b[0;32m---> 12\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m preprocess(img1)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(img1)\n",
      "File \u001b[0;32m~/Desktop/code/code/lib/python3.8/site-packages/PIL/Image.py:3087\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromarray\u001b[39m(obj, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;124;03m    Creates an image memory from an object exporting the array interface\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m    (using the buffer protocol)::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.6\u001b[39;00m\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3087\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array_interface__\u001b[49m\n\u001b[1;32m   3088\u001b[0m     shape \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3089\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__array_interface__'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "from sentence_transformers import util\n",
    "from PIL import Image\n",
    "\n",
    "# image processing model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
    "model.to(device)\n",
    "def imageEncoder(img):\n",
    "    img1 = Image.fromarray(img).convert('RGB')\n",
    "    img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "    img1 = model.encode_image(img1)\n",
    "    return img1\n",
    "def generateScore(image1, image2):\n",
    "    image1 = cv2.imread(\"blank.jpg\")\n",
    "    image2 = cv2.imread(\"add_text.jpg\")\n",
    "    #test_img = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n",
    "    #data_img = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n",
    "    img1 = imageEncoder(image1)\n",
    "    img2 = imageEncoder(image2)\n",
    "    cos_scores = util.pytorch_cos_sim(img1, img2)\n",
    "    score = round(float(cos_scores[0][0])*100, 2)\n",
    "    return score\n",
    "print(f\"similarity Score: \", round(generateScore(image1, image2), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@developerRegmi/image-similarity-comparison-using-vgg16-deep-learning-model-a663a411cd24#:~:text=Image%20Similarity%20Comparison%20using%20VGG16%20Deep%20Learning%20Model,-Roman&text=VGG16%20is%20a%20powerful%20pretrained,compare%20them%20to%20identify%20similarities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between images can then be computed based on the cosine similarity or Euclidean distance of these feature vectors. To improve the accuracy, we can preprocess the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main applications of the image similarity technique include e-commerce product matching, image retrieval, object recognition, and face recognition.\n",
    "\n",
    "Image similarity, for example, is used in image retrieval to find images similar to a query image.\n",
    "\n",
    "Image similarity can be used in object recognition to match a given object with a known database. An image similarity algorithm is used to identify people by comparing their faces to a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to measure image similarity is a vital component of numerous applications in today’s visually driven world. \n",
    "You can also explore the Siamese networks, a special class of neural networks designed for one-shot learning and image similarity tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
