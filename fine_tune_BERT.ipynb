{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google created a transformer-based machine learning approach for natural language processing pre-training called Bidirectional Encoder Representations from Transformers. It has a huge number of parameters, hence training it on a small dataset would lead to overfitting. This is why we use a pre-trained BERT model that has been trained on a huge dataset. Using the pre-trained model and try to “tune” it for the current dataset, i.e. transferring the learning, from that huge dataset to our dataset, so that we can “tune” BERT from that point onwards.\n",
    "\n",
    "In this article, we will fine-tune the BERT by adding a few neural network layers on our own and freezing the actual layers of BERT architecture. The problem statement that we are taking here would be of classifying sentences into POSITIVE and NEGATIVE by using fine-tuned BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.read_csv('datasets/sentiment_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['sentence'], df['label'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.3, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = df['label']) \n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state = 2021, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.5, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstratify = temp_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained BERT model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, we proceed with loading the pre-trained BERT model and tokenizer. We would use the tokenizer to convert the text into a format(which has input ids, attention masks) that can be sent to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load model and tokenizer \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bert \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer \n",
    "bert = AutoModel.from_pretrained('bert-base-uncased') \n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding the padding length\n",
    "If we take the padding length as the maximum length of text found in the training texts, it might leave the training data sparse. Taking the least length would in turn lead to loss of information. Hence, we would plot the graph and see the “average” length and set it as the padding length to trade-off between the two extremes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lens = [len(i.split()) for i in train_text] \n",
    "plt.hist(train_lens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we take 17 as the padding length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the data\n",
    "Tokenize the data and encode sequences using the BERT tokenizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences \n",
    "tokens_train = tokenizer.batch_encode_plus( \n",
    "\ttrain_text.tolist(), \n",
    "\tmax_length = pad_len, \n",
    "\tpad_to_max_length = True, \n",
    "\ttruncation = True\n",
    ") \n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus( \n",
    "\tval_text.tolist(), \n",
    "\tmax_length = pad_len, \n",
    "\tpad_to_max_length = True, \n",
    "\ttruncation = True\n",
    ") \n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus( \n",
    "\ttest_text.tolist(), \n",
    "\tmax_length = pad_len, \n",
    "\tpad_to_max_length = True, \n",
    "\ttruncation = True\n",
    ") \n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids']) \n",
    "train_mask = torch.tensor(tokens_train['attention_mask']) \n",
    "train_y = torch.tensor(train_labels.tolist()) \n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids']) \n",
    "val_mask = torch.tensor(tokens_val['attention_mask']) \n",
    "val_y = torch.tensor(val_labels.tolist()) \n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids']) \n",
    "test_mask = torch.tensor(tokens_test['attention_mask']) \n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model\n",
    "We first freeze the BERT pre-trained model, and then add layers as shown in the following code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the pretrained layers \n",
    "for param in bert.parameters(): \n",
    "\tparam.requires_grad = False\n",
    "\n",
    "#defining new layers \n",
    "class BERT_architecture(nn.Module): \n",
    "\n",
    "\tdef __init__(self, bert): \n",
    "\t\t\n",
    "\tsuper(BERT_architecture, self).__init__() \n",
    "\n",
    "\tself.bert = bert \n",
    "\t\t\n",
    "\t# dropout layer \n",
    "\tself.dropout = nn.Dropout(0.2) \n",
    "\t\t\n",
    "\t# relu activation function \n",
    "\tself.relu = nn.ReLU() \n",
    "\n",
    "\t# dense layer 1 \n",
    "\tself.fc1 = nn.Linear(768,512) \n",
    "\t\t\n",
    "\t# dense layer 2 (Output layer) \n",
    "\tself.fc2 = nn.Linear(512,2) \n",
    "\n",
    "\t#softmax activation function \n",
    "\tself.softmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "\t#define the forward pass \n",
    "\tdef forward(self, sent_id, mask): \n",
    "\n",
    "\t#pass the inputs to the model \n",
    "\t_, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False) \n",
    "\t\t\n",
    "\tx = self.fc1(cls_hs) \n",
    "\n",
    "\tx = self.relu(x) \n",
    "\n",
    "\tx = self.dropout(x) \n",
    "\n",
    "\t# output layer \n",
    "\tx = self.fc2(x) \n",
    "\t\t\n",
    "\t# apply softmax activation \n",
    "\tx = self.softmax(x) \n",
    "\n",
    "\treturn x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, add an optimizer to enhance the performance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr = 1e-5) # learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model \n",
    "def train(): \n",
    "\t\n",
    "model.train() \n",
    "\n",
    "total_loss, total_accuracy = 0, 0\n",
    "\t\n",
    "# empty list to save model predictions \n",
    "total_preds=[] \n",
    "\t\n",
    "# iterate over batches \n",
    "for step,batch in enumerate(train_dataloader): \n",
    "\t\n",
    "\t# progress update after every 50 batches. \n",
    "\tif step % 50 == 0 and not step == 0: \n",
    "\tprint(' Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader))) \n",
    "\n",
    "\t# push the batch to gpu \n",
    "\tbatch = [r.to(device) for r in batch] \n",
    "\n",
    "\tsent_id, mask, labels = batch \n",
    "\n",
    "\t# clear previously calculated gradients \n",
    "\tmodel.zero_grad()\t\t \n",
    "\n",
    "\t# get model predictions for the current batch \n",
    "\tpreds = model(sent_id, mask) \n",
    "\n",
    "\t# compute the loss between actual and predicted values \n",
    "\tloss = cross_entropy(preds, labels) \n",
    "\n",
    "\t# add on to the total loss \n",
    "\ttotal_loss = total_loss + loss.item() \n",
    "\n",
    "\t# backward pass to calculate the gradients \n",
    "\tloss.backward() \n",
    "\n",
    "\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "\t# update parameters \n",
    "\toptimizer.step() \n",
    "\n",
    "\t# model predictions are stored on GPU. So, push it to CPU \n",
    "\tpreds=preds.detach().cpu().numpy() \n",
    "\n",
    "\t# append the model predictions \n",
    "\ttotal_preds.append(preds) \n",
    "\n",
    "# compute the training loss of the epoch \n",
    "avg_loss = total_loss / len(train_dataloader) \n",
    "\t\n",
    "# predictions are in the form of (no. of batches, size of batch, no. of classes). \n",
    "total_preds = np.concatenate(total_preds, axis=0) \n",
    "\n",
    "#returns the loss and predictions \n",
    "return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define another function that would evaluate the model on validation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code \n",
    "print \"GFG\"\n",
    "# function for evaluating the model \n",
    "def evaluate(): \n",
    "\t\n",
    "print(\"\\nEvaluating...\") \n",
    "\t\n",
    "# deactivate dropout layers \n",
    "model.eval() \n",
    "\n",
    "total_loss, total_accuracy = 0, 0\n",
    "\t\n",
    "# empty list to save the model predictions \n",
    "total_preds = [] \n",
    "\n",
    "# iterate over batches \n",
    "for step,batch in enumerate(val_dataloader): \n",
    "\t\n",
    "\t# Progress update every 50 batches. \n",
    "\tif step % 50 == 0 and not step == 0: \n",
    "\t\t\n",
    "\t# # Calculate elapsed time in minutes. \n",
    "\t# elapsed = format_time(time.time() - t0) \n",
    "\t\t\t\n",
    "\t# Report progress. \n",
    "\tprint(' Batch {:>5,} of {:>5,}.'.format(step, len(val_dataloader))) \n",
    "\n",
    "\t# push the batch to gpu \n",
    "\tbatch = [t.to(device) for t in batch] \n",
    "\n",
    "\tsent_id, mask, labels = batch \n",
    "\n",
    "\t# deactivate autograd \n",
    "\twith torch.no_grad(): \n",
    "\t\t\n",
    "\t# model predictions \n",
    "\tpreds = model(sent_id, mask) \n",
    "\n",
    "\t# compute the validation loss between actual and predicted values \n",
    "\tloss = cross_entropy(preds,labels) \n",
    "\n",
    "\ttotal_loss = total_loss + loss.item() \n",
    "\n",
    "\tpreds = preds.detach().cpu().numpy() \n",
    "\n",
    "\ttotal_preds.append(preds) \n",
    "\n",
    "# compute the validation loss of the epoch \n",
    "avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "# reshape the predictions in form of (number of samples, no. of classes) \n",
    "total_preds = np.concatenate(total_preds, axis=0) \n",
    "\n",
    "return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data\n",
    "After fine-tuning the model, test it on the test dataset. Print a classification report to get a better picture of the model’s performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data \n",
    "with torch.no_grad(): \n",
    "preds = model(test_seq.to(device), test_mask.to(device)) \n",
    "preds = preds.detach().cpu().numpy() \n",
    "\t\n",
    "from sklearn.metrics import classification_report \n",
    "pred = np.argmax(preds, axis = 1) \n",
    "print(classification_report(test_y, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
