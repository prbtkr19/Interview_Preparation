{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Amazon SageMaker is a service that covers the entire machine learning workflow from data preprocessing to model deployment.\n",
    "\n",
    "Key benefits include:\n",
    "\n",
    "-   Fully managed infrastructure for training and hosting models\n",
    "-   Tight integration with other AWS services like S3, EC2, CloudWatch\n",
    "-   Support for popular frameworks like TensorFlow, PyTorch, scikit-learn\n",
    "T-  ools for monitoring, logging, scaling, and security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model deployment specifically, SageMaker offers:\n",
    "\n",
    "Serverless deployment architecture\n",
    "Automatic scaling of prediction capacity\n",
    "A/B testing capabilities\n",
    "Batch transform jobs for large volumes of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to Deploy a Model on SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Upload artifacts — Package model artifacts like weights and hyperparameters in a .tar.gz file and upload to S3.\n",
    "2. Create model — Use the SageMaker SDK to specify the S3 location of the artifacts, docker image, and compute requirements. This will create a SageMaker model resource.\n",
    "3. Deploy to endpoint — SageMaker deploys the model to an HTTPS endpoint which handles invocation and scaling.\n",
    "4. Make predictions — Send JSON requests to the endpoint URI to have your model run predictions in real-time.\n",
    "5. Monitor — View logs and metrics through CloudWatch to monitor throughput, latency, errors, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
